# American Sign Language Prediction Using Deep Learning Models

Sign language is a communication medium that uses facial/bodily expressions, postures and a set of gestures in human-human communication, as well as through TV and social networks. 
Sign language is utilized as the first language by millions of deaf people (hearing impaired people), in addition, hard-of-hearing people,
and people who have various speaking difficulties. In accordance with the investigation conducted by the British Deaf Association, it is recorded that about 151,000 
people use sign language as a means of communication. 
There is no universal sign language and almost every country has its own national sign language and fingerspelling alphabet. 
They use a combination of manual gestures with facial mimics and lips articulation. 
These sign languages have a special grammar that has fundamental differences to speech-based spoken languages.


Here, we Create an interpreter that recognizes sign language characters(alphabets) using pre-trained InceptionV3 and 
MobileNetV2 models with two additional dense layers of neurons for each model. InceptionV3 gives over 99% accuracy 
whereas MobileNetV2 gives around 96% accuracy in detecting the characters represented using hand symbols. 
Both have decent confusion matrix
